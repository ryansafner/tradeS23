{
  "hash": "d380a2fd641bf735ee88a26e56b0b07f",
  "result": {
    "markdown": "---\ntitle: \"Nonlinear Functions & Optimization\"\nexecute:\n  freeze: auto\n  echo: false\n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\nA function is non-linear if it is curved, i.e. not a straight line. Nonlinear functions' slopes may be different for different values of the independent variable.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](nonlinear_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\nThe slope at any particular point of the function is is its first derivative, the rate of instantaneous change.\n\n- Equivalently in practice, the value of $f'(x)$ is the slope of a line tangent to the function at point $(x_i, f(x_i))$\n\nMost applications in economics pertain to **marginal** magnitudes\n\n- Slopes mean change, and the margin implies a small change\n  - Often describe the **rate of substitution** between two goods (how much $y$ must you give up to get one more unit of $x$)\n- At the limit, marginal magnitudes are derivatives of a total magnitude\n  - e.g. **Marginal cost (or revenue)** is the derivative of *Total Cost (or revenue)* (and its slope at each value)\n  - e.g. **Marginal product** is the derivative of *Total Product* (and its slope at each value)\n\nWe can describe a curved function as being either **convex** or **concave** with respect to the origin (0,0)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](nonlinear_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIn simplest terms, a function is <span style=color:red;>concave</span> between two points $a, b$ if a straight line connecting $a$ and $b$ lies *beneath* the function itself\n$$\\color{red}{f[(t a)+(1-t)b]} > t f(a) + (1-t) f(b)\\text{ for }0 < t < 1$$\n\n- The above formula is a weighted average (for any set of weights $w$, $1-w$), implying that the weighted average of $a$ and $b$ (dotted line in graph) is below the function\n- The weighted average (dotted line) of $a$ and $b$ is below the function\n- A function is also concave at a point if its second derivative at that point is negative\n$$\\color{blue}{f[(w a)+(1-w)b]} < w f(a) + (1-w) f(b) \\quad\\text{ for }0 < w < 1$$\n\nIn simplest terms, a function is <span style=color:blue;>convex</span> between two points $a, b$ if a straight line connecting $a$ and $b$ lies *above* the function itself\n$$\\color{blue}{f[(w a)+(1-w)b]} < w f(a) + (1-w) f(b) \\quad \\text{ for }0 < w < 1$$\n  - The weighted average (dotted line) of $a$ and $b$ is above the function\n  - A function is also convex at a point if its second derivative is positive\n\nA function switches between convex and concave at an **inflection point** (point C in the example above)\n  - Here, the second derivative (in addition to the first) is equal to 0\n  \n## Optimization\n\nFor most curves, we often want to find the value where the function reaches its **maximum** or **minimum** (in general, these are types of “extrema”) along some interval\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](nonlinear_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nFormally, a function reaches a **maximum** at $x^*$ if $f(x^*) \\geq f(x)$ for all $x$; or a **minimum** at $x^*$ if $f(x^*) \\leq f(x)$ for all $x$\n  - In the graph above, the function reaches a maximum at $x_1$ and a minimum at $x_2$\n\nThe maximum or minimum of a function occurs where the slope (first derivative) is zero, known in calculus as the **“first-order condition”**\n$$\\frac{df(x^*)}{dx} = 0$$\n\n- To distinguish between maxima and minima, we have the **“second-order condition”**\n  - A *minimum* occurs when the *second* derivative of the function is positive, and the curve is *convex*\n$$\\frac{d^2f(x^*)}{dx^2} > 0$$\n  - A *maximum* occurs when the *second* derivative of the function is negative, and the curve is *concave*\n$$\\frac{d^2f(x^*)}{dx^2} < 0$$\n  - An *inflection point* occurs where the second derivative of the function is zero\n  - All three are known as “critical points”\n\nThis is often useful for **unconstrained optimization**, e.g. finding the quantity of output that maximizes profits\n\n*Note*, if we have a *multivariate* function $y=f(x_1, x_2)$ and want to find the maximum or minimum ($x_1^\\star, x_2^\\star$), the **first order conditions** (FOC, plural) are where all the **partial derivatives** (derivative with respect to $x_1$ and derivative with respect to $x_2$) are zero\n$$\\begin{align*}\n\\frac{\\partial f(x_1^*, x_2^*)}{\\partial x_1} &= 0 \\\\ \n\\frac{\\partial f(x_1^*, x_2^*)}{\\partial x_2} &= 0 \\\\ \n\\end{align*}$$\nThere are second order conditions as well, to demonstrate whether an extremum is a maximum or minimum, but they are too  complex to discuss here. \n\nOften we want to find the maximum or minimum of a function over some restricted values of $(x_1, x_2)$, known as **constrained optimization**. This is one of the most important modeling tools in microeconomics, and will show up in *many* contexts.\n\n- We want to find the maximum of some function:\n\n\\begin{align*}\n\\max_{x_1, x_2} f(x_1, x_2)\\\\\n\\text{subject to } g(x_1, x_2)=c \\\\\n\\end{align*}\n\n- $f(x_1, x_2)$ is the *“objective function”* we wish to maximize (or minimize)\n- $g(x_1, x_2)=c$ is the *“constraint”* that limits us within some specified set of $x_1$ and $x_2$ values\n  \nMuch of microeconomic modeling is about figuring out what an agent’s objective is (e.g. maximize profits, maximize utility, minimize costs) and what their constraints are (e.g. budget, time, output).\n\nThere are several ways to solve a constrained optimization problem (see Appendix to Ch. 5 in textbook), the most frequent (but requiring calculus) is Lagrangian multiplier method.\n\nGraphically, the solution to a constrained optimization problem is the point where a curve (objective function) and a line (constraint) are **tangent** to one another: they just touch, but do not intersect (e.g. at point A below).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](nonlinear_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nAt the point of tangency (A), **the slope of the curve (objective function) is *equal* to the slope of the line (constraint)**\n\n- This is extremely useful and is always the solution to simple **constrained optimization** problems, e.g.\n    - e.g. maximizing utility subject to income\n    - e.g. minimizing cost subject to a certain level of output\n\n- We can find the equation of the tangent line using point slope form\n$$y-y_1=m(x-x_1)$$\n- We need to know the slope $m$, which we would know from the slope of the function at that point\n- We know $(x_1, y_1$) is the point of tangency\n",
    "supporting": [
      "nonlinear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}